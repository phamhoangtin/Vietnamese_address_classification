# -*- coding: utf-8 -*-
"""Copy of GettingStarted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dXTq5XArbzOzoqoxJP-L6oUamf3RwXUB
"""

# %%bash
# # NOTE: you CAN change this cell
# # Add more to your needs
# # you must place ALL pip install here
# pip install numpy
# pip install pandas
# pip install editdistance

# NOTE: you CAN change this cell
# import your library here
import time
import re
from collections import Counter
import pandas as pd
import json

"""## support class"""


class auto_correct_1_word:
    def __init__(self, file_paths):
        if type(file_paths) is str:
            with open(file_paths, 'r', encoding="utf-8") as file:
                all_text = file.read()
            self.WORDS = Counter(self.words(all_text))
        elif type(file_paths) is list:
            all_text = ''
            for file_path in file_paths:
                with open(file_path, 'r', encoding="utf-8") as file:
                    all_text += file.read()
            self.WORDS = Counter(self.words(all_text))

    def words(self, text):
        return re.findall(r'\w+', text.lower())

    def P(self, word):
        "Probability of `word`."
        N = sum(self.WORDS.values())
        return self.WORDS[word] / N

    def correction(self, word):
        "Most probable spelling correction for word."
        return max(self.candidates(word), key=self.P)

    def candidates(self, word):
        "Generate possible spelling corrections for word."
        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])

    def known(self, words):
        "The subset of `words` that appear in the dictionary of WORDS."
        return set(w for w in words if w in self.WORDS)

    def edits1(self, word):
        "All edits that are one edit away from `word`."
        letters = 'aăâbcdeêfghikmnoôơpqrstuưvwxyz'
        # letters    = 'aàáảăằắẳẵâầấẩẫậbcdefghijklmnopqrstưuvwxyz'
        # letters = 'aàáảãạăằắẳẵặâầấẩẫậbcdđeèéẻẽẹêềếểễệghiìíỉĩịklmnoòóỏõọôồốổỗộơờớởỡợpqrstuùúủũụưừứửữựvxyz'
        # letters = 'aàáảãạăằắẳẵặâầấẩẫậbcdđeèéẻẽẹêềếểễệghiìíỉĩịklmnoợpqrstuựvxyz'
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        deletes = [L + R[1:] for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
        inserts = [L + c + R for L, R in splits for c in letters]
        return set(deletes + transposes + replaces + inserts)

    def edits2(self, word):
        "All edits that are two edits away from `word`."
        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))


class TrieNode:
    def __init__(self, _character=None):
        self.children = {}
        self.character = _character
        self.is_end_of_word = False


class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word):
        word_lower = word.lower()
        word_lower = word_lower.replace("oá", "óa")
        word_lower = word_lower.replace("oà", "òa")
        work_remove_space = word_lower.replace(" ", "")
        # list_word = [word_lower,work_remove_space]
        list_word = [work_remove_space]
        for word_type in list_word:
            current_node = self.root
            for char in word_type:
                node = current_node.children.get(char)
                if node is None:
                    node = TrieNode(_character=char)
                    current_node.children.update({char: node})
                current_node = node
            current_node.is_end_of_word = True

    def search(self, word):
        word = word.replace("oá", "óa")
        word = word.replace("oà", "òa")
        node = self.root
        prefix = ''
        list_space_position = []
        for i in range(len(word)):
            if word[i] == " ":
                list_space_position.append(i)
        word = word.lower().replace(" ", "")
        for char in word:
            if char not in node.children:
                return {'is_find': False, 'prefix': prefix.strip()}
            prefix += char
            node = node.children[char]

        for pos in sorted(list_space_position):
            prefix = prefix[:pos] + ' ' + prefix[pos:]
        return {'is_find': node.is_end_of_word, 'prefix': prefix.strip()}

    def find_nearest_word(self, word):

        list_same_prefix = self.dfs_search_with_prefix(word)
        nearest_word = None
        min_distance = float('inf')
        for word_same_prefix in list_same_prefix:
            distance = self.levenshtein_distance(word_same_prefix, word)
            if distance < min_distance:
                nearest_word = word_same_prefix
                min_distance = distance
        return nearest_word

    def dfs_search_with_prefix(self, prefix):
        node = self.root
        for char in prefix:
            if char not in node.children:
                # Prefix not found in the trie
                return []
            node = node.children[char]
        # Perform DFS to find all words with the given prefix
        words = []
        self._dfs(node, prefix, words)
        return words

    def _dfs(self, node, current_word, words):
        if node.is_end_of_word:
            words.append(current_word)
        for char, child_node in node.children.items():
            self._dfs(child_node, current_word + char, words)

    def levenshtein_distance(self, str1, str2):
        len_str1 = len(str1)
        len_str2 = len(str2)

        # Create a matrix to store the distances
        matrix = [[0] * (len_str2 + 1) for _ in range(len_str1 + 1)]

        # Initialize the matrix with the distances from each string to an empty string
        for i in range(len_str1 + 1):
            matrix[i][0] = i
        for j in range(len_str2 + 1):
            matrix[0][j] = j

        # Populate the matrix with the minimum distances
        for i in range(1, len_str1 + 1):
            for j in range(1, len_str2 + 1):
                cost = 0 if str1[i - 1] == str2[j - 1] else 1
                matrix[i][j] = min(
                    matrix[i - 1][j] + 1,  # Deletion
                    matrix[i][j - 1] + 1,  # Insertion
                    matrix[i - 1][j - 1] + cost  # Substitution
                )
        # The bottom-right cell contains the Levenshtein distance
        return matrix[len_str1][len_str2]


"""## support function"""


def replace_str(s):
    if s:
        s = s.replace('oà', 'òa')
        s = s.replace('oá', 'óa')
    return s


def file_to_list(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        content_list = [line.strip() for line in file]
    return content_list


def split_str(string):
    capBannedWords = ["H.", "T.", "H ", "T "]
    for capBannedWord in capBannedWords:
        string = string.replace(capBannedWord, " ")
    string = string.lower()
    WORDS = re.compile(r"\w+\s*[\.]?\w+")
    bannedWords = ["huyện", "thị trấn", "thị xã", "tt", "tỉnh", "tphố", "tp.", "tp ", "t.p", 'h. ', 'x.', 't.phốbuôn',
                   't.phố', 'thành phố', 'phố ', 't.x', 't.', 'x ', 'tx.', 'Fhố', 'fhố', "xã", "xã "]
    for bannedWord in bannedWords:
        string = string.replace(bannedWord, "")
    string = string.replace(" hn", " hà nội")
    string = string.replace("hcm", "hồ chí minh")
    words = WORDS.findall(string[::-1])
    return [word[::-1] for word in words]


def insert_to_trie(trie, list_word):
    for word in list_word:
        trie.insert(word)


with open('data/all_3_or_more.txt', 'r', encoding="utf-8") as file:
    content_list = file.readlines()
combined_list = [line.strip() for line in content_list]


def correct_text(correction_obj, list_text):
    # list_text = re.split(r'[ ,;]', str_text)
    list_result = []
    for text in list_text:
        list_normalize = []
        for word in text.split():
            list_normalize.append(correction_obj.correction(word))

        list_result.append(' '.join(list_normalize))
    # gom nhom chi minh ho
    i = 0
    while i < len(list_result) - 1:
        temp_name = list_result[i + 1] + " " + list_result[i]
        temp_name = temp_name.replace("huyện ", "")
        temp_name = temp_name.replace("phường ", "")
        temp_name = temp_name.replace("xã ", "")
        temp_name = temp_name.replace("thị xã ", "")
        if (len(temp_name) > 2) and (temp_name in combined_list):
            del list_result[i + 1]
            list_result[i] = temp_name
        i += 1
    return list_result


def replace_value_dict(look_up, dict):
    if look_up:
        if look_up in dict:
            look_up = dict[look_up]
        elif look_up == 'bà rịa vũng tàu':
            look_up = 'bà rịa - vũng tàu'
        return look_up.title()
    return look_up


def convert_to_json(out_put_path='output.json', data_frame=None):
    if ".json" not in out_put_path:
        out_put_path = out_put_path + '/output.json'

    ls_result = []
    for index, row in data_frame.iterrows():
        result_dict = {}
        text = row['text']
        province = row['province_1']
        district = row['district_1']
        ward = row['ward_1']
        result_dict['text'] = text
        result_dict['result'] = {'province': province, 'district': district, 'ward': ward}
        ls_result.append(result_dict)
    # Write the list of dictionaries to a JSON file
    with open(out_put_path, 'w+') as json_file:
        json.dump(ls_result, json_file, indent=2, ensure_ascii=False)


"""## solution"""


# NOTE: you MUST change this cell

class Solution:
    def __init__(self):
        # list provice, district, ward for private test
        self.province_path = 'list_province.txt'
        self.district_path = 'list_district.txt'
        self.ward_path = 'list_ward.txt'
        self.w_all = self.create_auto_complete()
        self.address_dict = self.create_address_list()[0]
        self.trie_ward = self.create_address_list()[1]
        self.trie_district = self.create_address_list()[2]
        self.trie_province = self.create_address_list()[3]

    def create_auto_complete(self):
        # standard
        file_path_ward_standard = "data/ward.txt"
        file_path_district_standard = "data/district.txt"
        file_path_province_standard = "data/province.txt"

        file_path_ward_teacher = self.ward_path
        file_path_district_teacher = self.district_path
        file_path_province_teacher = self.province_path
        # create combine list file
        list_file = [file_path_ward_standard, file_path_district_standard, file_path_province_standard, \
                     file_path_province_teacher, file_path_district_teacher, file_path_ward_teacher]

        w_all = auto_correct_1_word(list_file)
        return w_all

        # write your preprocess here, add more method if needed

    def create_address_list(self):
        file_path_ward_standard = "data/ward.txt"
        file_path_district_standard = "data/district.txt"
        file_path_province_standard = "data/province.txt"

        file_path_ward_teacher = self.ward_path
        file_path_district_teacher = self.district_path
        file_path_province_teacher = self.province_path
        # create combine list file
        list_file = [file_path_ward_standard, file_path_district_standard, file_path_province_standard, \
                     file_path_province_teacher, file_path_district_teacher, file_path_ward_teacher]

        # create list standard
        list_ward_standard = file_to_list(file_path_ward_standard)
        list_district_standard = file_to_list(file_path_district_standard)
        list_province_standard = file_to_list(file_path_province_standard)
        # create list by teacher provide
        list_ward_teacher = file_to_list(file_path_ward_teacher)
        list_district_teacher = file_to_list(file_path_district_teacher)
        list_province_teacher = file_to_list(file_path_province_teacher)
        # create combine list
        list_province = list_province_standard
        list_district = list_district_standard
        list_ward = list_ward_standard

        list_pro_dis_war = set(list_province + list_district + list_ward)
        address_dict = {item.lower().replace(" ", ""): item.lower() for item in list_pro_dis_war}

        # build trie_1
        trie_ward = Trie()
        insert_to_trie(trie_ward, list_ward)
        trie_district = Trie()
        insert_to_trie(trie_district, list_district)
        trie_province = Trie()
        insert_to_trie(trie_province, list_province)
        return (address_dict, trie_ward, trie_district, trie_province)

    def test_trie_1(self, ls_add):
        result = {
            "province": "",
            "district": "",
            "ward": ""}
        list_trie = [self.trie_province, self.trie_district, self.trie_ward]
        list_unit = ['province', 'district', 'ward']
        dic_check_update_result = {"province": 0,
                                   "district": 0,
                                   "ward": 0}
        counter_enough = 0
        for address in ls_add:
            for i in range(3):
                if counter_enough >= 3:
                    break
                trie_result = list_trie[i].search(address)

                if trie_result['is_find'] and dic_check_update_result[list_unit[i]] == 0:
                    result[list_unit[i]] = trie_result['prefix']
                    dic_check_update_result[list_unit[i]] = 1
                    counter_enough += 1

                    break
        if counter_enough < 3 and len(ls_add) >= 3:

            if dic_check_update_result['province'] == 0:
                trie_result_prefix = self.trie_province.search(ls_add[0])['prefix']
                nestest_word = self.trie_province.find_nearest_word(trie_result_prefix)
                result["province"] = nestest_word

            if dic_check_update_result['district'] == 0:
                index = 1
                trie_result_prefix = self.trie_district.search(ls_add[index])['prefix']
                nestest_word = self.trie_district.find_nearest_word(trie_result_prefix)
                result["district"] = nestest_word
            if dic_check_update_result['ward'] == 0:
                index = 2
                trie_result_prefix = self.trie_ward.search(ls_add[index])['prefix']
                nestest_word = self.trie_ward.find_nearest_word(trie_result_prefix)
                result["ward"] = nestest_word

        return result

    def process(self, s: str):
        ls_str_slipt = split_str(s)
        ls_str_correct = correct_text(self.w_all, ls_str_slipt)
        dict_semi_result = self.test_trie_1(ls_str_correct)

        result = dict()
        result['province'] = replace_value_dict(dict_semi_result['province'], self.address_dict)
        result['district'] = replace_value_dict(dict_semi_result['district'], self.address_dict)
        result['ward'] = replace_value_dict(dict_semi_result['ward'], self.address_dict)
        # write your process string here
        return result


"""## testing"""

# NOTE: you DO NOT NEED TO change this cell
# Scoring
import json
import time

# with open('test.json') as f:
#     data = json.load(f)
with open('data/public_new.json') as f:
    data = json.load(f)
summary_only = True
solution = Solution()

timer = []
correct = 0
for test_idx, data_point in enumerate(data):
    address = data_point["text"]

    ok = 0
    try:
        start = time.perf_counter_ns()
        result = solution.process(address)
        answer = data_point["result"]
        finish = time.perf_counter_ns()
        timer.append(finish - start)
        ok += int(answer["province"] == result["province"])
        ok += int(answer["district"] == result["district"])
        ok += int(answer["ward"] == result["ward"])
    except Exception as e:
        print(str(e))
        # any failure count as a zero correct
        pass
    correct += ok

    if not summary_only:
        # responsive stuff
        print(f"Test {test_idx:5d}/{len(data):5d}")
        print(f"Correct: {ok}/3")
        print(f"Time Executed: {timer[-1] / 1_000_000_000:.4f}")

print(f"-" * 30)
total = len(data) * 3
print(f"correct = {correct} / {total} ({correct / total * 10:.2f} / 10) ")
if len(timer) == 0:
    timer = [0]
print(f"max_time_sec = {max(timer) / 1_000_000_000:.4f}")
print(f"avg_time_sec = {(sum(timer) / len(timer)) / 1_000_000_000:.4f}")
